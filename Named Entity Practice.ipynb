{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2598f3",
   "metadata": {},
   "source": [
    "# Trying Custom NER through a TDS Article and Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c9cc3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports needed for the training and testing of the model\n",
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb1e7c",
   "metadata": {},
   "source": [
    "First we must read in a dataset that we will train and test the model with. The dataset used can be found [here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f14121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ner_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b27730de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d25b3a",
   "metadata": {},
   "source": [
    "This function is a helper function to help us find the nth instance of a word in a sentence. This will help us format the data for training the way spacy wants it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3b8a702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findnth(string, substring, n):\n",
    "    parts = string.split(substring, n + 1)\n",
    "    if len(parts) <= n + 1:\n",
    "        return -1\n",
    "    return len(string) - len(parts[-1]) - len(substring)\n",
    "# findnth('Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .', 'the', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdba34a",
   "metadata": {},
   "source": [
    "This function formats the data from the .csv to a list of tuples that contain the information needed for the spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7331ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df,itns):\n",
    "    ret = [] #This will be the retured list of all the training data\n",
    "    sentence_col = list(df[~df['Sentence #'].isnull()].index) #this gets all the indices of starts of sentences\n",
    "    for i in range(itns): #get itns number of sentences for training\n",
    "        sentence = ' '.join(df['Word'][sentence_col[i]:sentence_col[i+1]]) #one sentence\n",
    "        dictionary = {} #this dictionary will contain the entities and where they start and where they end.\n",
    "        dictionary['entities'] = [] #this list contains the tuples of entities, start index, and end index.\n",
    "        tags = df[sentence_col[i]:sentence_col[i+1]][df['Tag'][sentence_col[i]:sentence_col[i+1]] != 'O'] #This will get all the instances of entities in the sentence\n",
    "        t = set() # a set that will contain tuples\n",
    "        for j in tags.index:\n",
    "            inEnts = False #boolean that checks to see if the entity was already found\n",
    "            it = 0\n",
    "            w = tags['Word'][j]\n",
    "            \n",
    "            while(inEnts == False): #With the given word, find the right instance of the word in the sentence\n",
    "                tup = (findnth(sentence,w,it),findnth(sentence,w,it) + len(w),tags['Tag'][j])\n",
    "                tu = (tup[0],tup[1])\n",
    "                \n",
    "                if(tu not in t):\n",
    "                    dictionary['entities'].append(tup)\n",
    "                    inEnts = True\n",
    "                    t.add(tu) #tuple added to the dictionary.\n",
    "#                     if(tu == (36,42) and i == 83):\n",
    "#                         print(\"Hello\")\n",
    "                else:\n",
    "                    it+=1\n",
    "#         if(i == 83):\n",
    "#             print(t)\n",
    "        tup2 = (sentence, dictionary) #The tuple that is appended to the list\n",
    "        ret.append(tup2)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8084488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "{(113, 118), (36, 42), (51, 59), (23, 25), (119, 125), (137, 143), (0, 8)}\n"
     ]
    }
   ],
   "source": [
    "x = format_data(df,1000) #store the formatted data in a variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82e26b",
   "metadata": {},
   "source": [
    "These are more imports needed for the training and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "09e484ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f48ddc",
   "metadata": {},
   "source": [
    "## Spacy v3.0 Addition\n",
    "Because of the new version of spacy, the old formatting of data will not work for training, so we will need to reformat the old format into a new .spacy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "189f247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(TRAIN_DATA):\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "    s = set()\n",
    "    j=0\n",
    "    for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    #     print(j)\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for i in text.split(' '):\n",
    "            s.add(i)\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "        j+=1\n",
    "\n",
    "    db.to_disk(\"./train.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "64802b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████▋                                                        | 269/1000 [00:00<00:00, 1349.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████▋                                  | 554/1000 [00:00<00:00, 1374.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1437.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4026d5",
   "metadata": {},
   "source": [
    "Now, we have the training data loaded up in our directory.\n",
    "Next, we must create a config file so that we can run the training and testing using the spacy CLI tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6ff2f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] To generate a more effective transformer-based config (GPU-only), install\n",
      "the spacy-transformers package and re-run this command. The config generated now\n",
      "does not use transformers.\n",
      "[i] Generated config template specific for your use case\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "\\content\\ner_demo\\configs\\config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config --lang en --pipeline ner /content/ner_demo/configs/config.cfg --force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cface561",
   "metadata": {},
   "source": [
    "This should give us the default configuration file for spacy ner. Next, we must split data to get test data, and format this data to be used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fdcbea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(147, 155), (5, 11)}\n"
     ]
    }
   ],
   "source": [
    "df1 = df[3000:]\n",
    "x1 = format_data(df1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "31329474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_test(TRAIN_DATA):\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "    s = set()\n",
    "    j=0\n",
    "    for text, annot in tqdm(TRAIN_DATA): # data in previous format\n",
    "    #     print(j)\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for i in text.split(' '):\n",
    "            s.add(i)\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "        j+=1\n",
    "\n",
    "    db.to_disk(\"./test.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "03f70aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████                                                         | 261/1000 [00:00<00:00, 1297.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████████                          | 663/1000 [00:00<00:00, 1263.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1340.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_test(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0a199",
   "metadata": {},
   "source": [
    "Now we can test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "90a91ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Using CPU\n",
      "\u001b[1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-03 23:27:48,935] [INFO] Set up nlp object from config\n",
      "[2021-06-03 23:27:48,941] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-06-03 23:27:48,945] [INFO] Created vocabulary\n",
      "[2021-06-03 23:27:48,945] [INFO] Finished initializing nlp object\n",
      "[2021-06-03 23:27:49,807] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     54.09    2.47    2.10    3.00    0.02\n",
      "  0      10          3.17    781.27    0.00    0.00    0.00    0.00\n",
      "  0      20          8.47    252.79    0.00    0.00    0.00    0.00\n",
      "  0      30          3.68    195.59    1.65    7.87    0.92    0.02\n",
      "  0      40          3.69    184.73   15.91   28.11   11.09    0.16\n",
      "  0      50          1.90    130.07   14.30   18.15   11.80    0.14\n",
      "  0      60          3.29    135.75   26.29   27.87   24.88    0.26\n",
      "  0      70          3.99    161.53   29.89   31.96   28.06    0.30\n",
      "  0      80          1.83    109.76   25.78   32.17   21.51    0.26\n",
      "  0      90          3.49    118.67   33.02   34.23   31.89    0.33\n",
      "  0     100          3.74    128.74   37.83   36.80   38.91    0.38\n",
      "  0     110          3.28    133.19   41.34   40.82   41.88    0.41\n",
      "  0     120          4.07    114.48   46.90   47.46   46.35    0.47\n",
      "  0     130          5.44    130.26   51.24   51.99   50.52    0.51\n",
      "  0     140          4.07     95.74   53.81   55.48   52.24    0.54\n",
      "  0     150          3.89     93.25   56.18   56.77   55.61    0.56\n",
      "  0     160         27.58    113.27   59.70   60.90   58.55    0.60\n",
      "  0     170          6.09    111.54   57.57   57.57   57.57    0.58\n",
      "  0     180          6.13    107.17   57.78   58.25   57.32    0.58\n",
      "  0     190          7.30    127.25   63.00   62.50   63.51    0.63\n",
      "  0     200          5.18    107.47   60.39   60.13   60.66    0.60\n",
      "  0     210          5.70     97.92   61.09   61.85   60.36    0.61\n",
      "  0     220          6.28    107.60   62.46   65.07   60.05    0.62\n",
      "  1     230          6.03     82.98   65.11   65.89   64.34    0.65\n",
      "  1     240        123.87    126.41   66.69   66.22   67.16    0.67\n",
      "  1     250         13.15    107.13   68.00   69.75   66.33    0.68\n",
      "  1     260          4.55     64.37   68.45   70.40   66.61    0.68\n",
      "  1     270          9.14     96.28   68.09   67.36   68.84    0.68\n",
      "  1     280          7.04     73.66   70.79   71.34   70.25    0.71\n",
      "  1     290          8.51     89.64   70.26   72.14   68.47    0.70\n",
      "  1     300          9.28     97.05   72.66   72.77   72.55    0.73\n",
      "  1     310          6.30     76.12   73.63   75.30   72.03    0.74\n",
      "  1     320          9.40     92.89   73.89   74.48   73.31    0.74\n",
      "  1     330          7.81     73.50   73.97   74.16   73.77    0.74\n",
      "  1     340         11.95     96.87   75.03   75.88   74.20    0.75\n",
      "  1     350          8.14     70.34   76.17   77.94   74.48    0.76\n",
      "  1     360         10.46     88.70   76.14   76.61   75.67    0.76\n",
      "  1     370          7.21     72.22   75.36   74.46   76.29    0.75\n",
      "  1     380          7.05     68.15   75.26   75.68   74.85    0.75\n",
      "  1     390         12.04     99.86   76.28   76.21   76.35    0.76\n",
      "  1     400         11.37     90.38   75.35   74.63   76.07    0.75\n",
      "  2     410          8.09     74.14   76.78   77.70   75.89    0.77\n",
      "  2     420          5.72     56.88   77.54   77.92   77.18    0.78\n",
      "  2     430          6.01     57.41   76.18   76.19   76.16    0.76\n",
      "  2     440         12.82     63.44   78.29   79.04   77.54    0.78\n",
      "  2     450          8.25     55.78   78.05   78.98   77.14    0.78\n",
      "  2     460          8.91     52.81   78.31   78.69   77.94    0.78\n",
      "  2     470         14.50     90.36   77.93   78.25   77.60    0.78\n",
      "  2     480          9.23     60.92   78.85   79.18   78.52    0.79\n",
      "  2     490         11.60     78.60   79.16   79.31   79.01    0.79\n",
      "  2     500         10.08     73.29   78.30   78.27   78.34    0.78\n",
      "[+] Saved pipeline to output directory\n",
      "\\training\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train /content/ner_demo/configs/config.cfg --output /training/ --paths.train train.spacy --paths.dev test.spacy --training.eval_frequency 10 --training.max_steps 500 --gpu-id -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a2b38",
   "metadata": {},
   "source": [
    "We see that with 500 iterations, we reach an F1 score of 78.30 and a score of 78, which is quite good. Now, we can see test this model on our own examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0ea85a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = spacy.load(R\"\\training\\model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "fbb4f09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-per</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Lee\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-per</span>\n",
       "</mark>\n",
       " is the chief of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CBSE\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-geo</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp1(\"John Lee is the chief of CBSE.\")\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "99507a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Americans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-gpe</span>\n",
       "</mark>\n",
       " suffered from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    H5N1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-nat</span>\n",
       "</mark>\n",
       " virus in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2002\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-tim</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp1 = spacy.load(R\"\\training\\model-best\")\n",
    "doc = nlp1(\"Americans suffered from H5N1 virus in 2002.\")\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
